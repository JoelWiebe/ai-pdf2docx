# AI PDF to DOCX Converter (ai-pdf2docx)

This tool leverages Google's Vertex AI Gemini models to convert PDF documents, particularly academic papers, into structured Microsoft Word (.docx) files. It intelligently extracts headings (H1, H2, H3), paragraphs, and tables, applying basic styling to the output DOCX.

The process is broken into two distinct phases:
1.  **Phase 1 (PDF to JSON):** Analyzes the PDF using a Gemini model and saves the extracted structure and content as a JSON file.
2.  **Phase 2 (JSON to DOCX):** Converts the structured JSON data into a formatted .docx file.

## Features

* **AI-Powered Extraction:** Uses advanced multimodal AI (Gemini on Vertex AI) to understand document structure.
* **Two-Phase Processing:**
    * `pdf2json`: Converts PDFs to an intermediary structured JSON format.
    * `json2docx`: Converts the JSON files to styled DOCX files.
* **Structure Recognition:** Identifies and extracts:
    * Heading levels (H1, H2, H3)
    * Paragraphs
    * Tables (converted from Markdown representation)
* **Basic DOCX Styling:** Applies styles for headings and paragraphs in the generated DOCX.
* **Command-Line Interface:** Easy to use via a wrapper script `ai-pdf2docx.py`.
* **Batch Processing:** Can process multiple PDF files from a directory.
* **Configurable:** Allows selection of Gemini model, GCP project, location, and processing delay.
* **Overwrite Control:** Option to overwrite existing output files.
* **VS Code Debugging:** Includes a `launch.json` for easy debugging in Visual Studio Code.

## Prerequisites

* Python 3.8 or higher.
* A Google Cloud Platform (GCP) project with:
    * Vertex AI API enabled.
    * Billing enabled.
* Google Cloud SDK (`gcloud` CLI) installed and authenticated with Application Default Credentials:
    ```bash
    gcloud auth application-default login
    ```
* The `python-docx` and `google-cloud-aiplatform` Python libraries.

## Setup & Installation

1.  **Clone the Repository:**
    ```bash
    git clone <your-repository-url>
    cd <your-repository-name>
    ```

2.  **Create and Activate a Virtual Environment (Recommended):**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # On Windows: .venv\Scripts\activate
    ```

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Configuration

When running the scripts, you'll need to provide your Google Cloud Project ID. You can also specify the GCP location and the Gemini model to use.

* **`--project_id YOUR_GCP_PROJECT_ID`**: (Required for `pdf2json`) Your Google Cloud project ID.
* **`--location YOUR_GCP_LOCATION`**: (Optional, defaults to `global` for `pdf2json`) The GCP region for Vertex AI.
* **`--model YOUR_MODEL_NAME`**: (Optional, defaults to `gemini-2.5-pro-preview-05-06` for `pdf2json`) The Gemini model to use.

## Usage

The primary way to use the tool is via the `ai-pdf2docx.py` wrapper script, which controls the two processing phases.

### Phase 1: PDF to JSON (`pdf2json`)

This command processes all PDF files in an input directory, sends each to the Gemini model for analysis, and saves the model's raw structured output as a `.json` file in the specified output directory.

```bash
python ai-pdf2docx.py pdf2json \
  ./pdf_input_folder \
  ./json_output_folder \
  --project_id "YOUR_GCP_PROJECT_ID" \
  --location "global" \
  --model "gemini-2.5-pro-preview-05-06" \
  --delay 1 \
  # --overwrite # Uncomment to overwrite existing JSON files
  ```

* **`./pdf_input_folder`**: Directory containing your source PDF files.
* **`./json_output_folder`**: Directory where the generated JSON files will be saved.
* **`--delay 1`**: Adds a 1-second delay between API calls to help manage rate limits. Adjust as needed.

### Phase 2: JSON to DOCX (`json2docx`)
This command processes all .json files (generated by Phase 1) in an input directory and converts each into a formatted .docx file in the specified output directory.

```bash
python ai-pdf2docx.py json2docx \
  ./json_output_folder \
  ./docx_output_folder \
  # --overwrite # Uncomment to overwrite existing DOCX files
```

* `./json_output_folder`: Directory containing the JSON files from Phase 1.
* `./docx_output_folder`: Directory where the generated DOCX files will be saved.

### VS Code Debugging

This workspace includes a `.vscode/launch.json` file with pre-configured launch profiles for debugging both the `pdf2json` and `json2docx` phases using the `ai-pdf2docx.py` wrapper.

1.  Open the project folder in VS Code.
2.  Go to the "Run and Debug" view (Ctrl+Shift+D or Cmd+Shift+D).
3.  Open the `.vscode/launch.json` file and **update the placeholder paths and your GCP Project ID** in the `args` section for each configuration.
4.  Select the desired launch configuration (e.g., "AI-PDF2DOCX: PDF to JSON") from the dropdown.
5.  Set breakpoints in the Python code and start debugging (F5).

### Important Considerations & Warnings

#### Vertex AI Costs
* **Using the Vertex AI Gemini models will incur costs on your Google Cloud Platform account.** Costs are typically based on the amount of data processed (e.g., number of characters or tokens in input and output) and the specific model used.
* Please review the official [Vertex AI Pricing page](https://cloud.google.com/vertex-ai/pricing) for the most up-to-date information before processing a large number of documents.
* Monitor your GCP billing dashboard regularly.

#### Computation Time
* The PDF to JSON conversion (Phase 1) can be time-consuming, especially for large and complex documents.
* **Estimate:** A 20-page research paper PDF might take approximately **6 minutes** for the `pdf2json` phase. This is a rough estimate and can vary significantly based on:
    * PDF complexity (scanned vs. text-based, number of images, intricate layouts).
    * The specific Gemini model version used.
    * Current Vertex AI service load.
    * Document length and content density.
* The `json2docx` phase (Phase 2) is generally much faster as it involves local processing.

#### API Rate Limits
* Google Cloud enforces API rate limits. If you are processing a very large batch of PDFs, you might encounter these limits.
* The `pdf2json` command includes a `--delay` option (e.g., `--delay 1` for a 1-second pause between files) which can help mitigate this. Adjust the delay as needed.

#### Accuracy & Output Quality
* AI-based document understanding is a complex task. While Gemini models are powerful, the accuracy of extracted headings, paragraphs, and especially tables, may vary.
* **Always review the generated DOCX files for accuracy and completeness.**
* **JSON Output (Phase 1):** The script saves the raw text output from the Gemini model as a `.json` file. While the prompt strongly requests valid JSON, there's a small chance the model might produce slightly malformed JSON for highly complex or unusual inputs. Phase 2 will report errors if it cannot parse a JSON file. You may need to manually inspect and correct such files.
* **Table Conversion:** Tables are extracted as Markdown and then converted to DOCX tables. Very complex tables (e.g., with merged cells or highly irregular structures) might not be rendered perfectly.

## Error Handling and Resuming Processing

The script is designed to save any successfully processed data before exiting due to an unrecoverable error (like repeated API failures or critical issues with a specific document). The output Excel file will be named with a suffix like `_ERROR_INCOMPLETE` or `_USER_INTERRUPTED_PARTIAL` in such cases.

If an error occurs and the script halts:

1.  **Identify Processed Files:**
    * Open the partially saved Excel workbook (e.g., `extracted_data_..._ERROR_INCOMPLETE.xlsx`).
    * Note the unique filenames listed in the "filename" column. These documents were successfully processed (at least up to the point of extraction for the variables listed).
    * Move these corresponding `.docx` files from your `input_docs` directory to a new, separate subfolder (e.g., `input_docs/Completed/`).

2.  **Identify the Problematic File:**
    * The console output when the script halted will typically indicate the last file it was attempting to process when the error occurred (e.g., `Processing was halted due to an error: File: problematic_document.docx, Critical Error: ...`).
    * Move this specific problematic `.docx` file from `input_docs` to a different new subfolder (e.g., `input_docs/Pending_Review/`).

3.  **Resume Processing Remaining Files:**
    * Run the `ai_data_extractor.py` script again. Since the successfully processed files and the identified problematic file have been moved out of the `input_docs` directory, the script will continue with the remaining unprocessed DOCX files.

4.  **Address Problematic Files:**
    * Once all other files are successfully processed (and their DOCX files also moved to your `input_docs/Completed/` folder), you can focus on the file(s) in `input_docs/Pending_Review/`.
    * Move one problematic file at a time back into the main `input_docs` directory.
    * Try running the script again, focusing on this single file.
    * **Troubleshooting:**
        * Check the console output for specific error messages related to this file.
        * Consider if the `MAX_TOKENS` limit was hit (see "API Rate Limits & Errors" section above). You might need to adjust `max_output_tokens` in `config.py` temporarily or consider if the document section is exceptionally large for the classification/extraction task.
        * Examine the DOCX file itself for any unusual formatting or potential corruption.
        * If the issue seems to be with how the LLM is responding (e.g., consistently malformed JSON, incorrect labels despite clear prompts), you might need to debug the prompt engineering for that specific type of content or report the issue if it seems like a bug in the script's logic.
        * You can also reduce `MAX_INVALID_LABEL_WARNINGS_PER_DOC` in `config.py` to a low number (like 0 or 1) to make the script stop more quickly if the issue is related to the "Classified label '...' is not a predefined paragraph tag" warnings, helping you pinpoint which section is causing the model to respond unexpectedly.

By following these steps, you can manage errors, ensure already processed data is saved, and systematically work through a large batch of documents.

### File Structure

* `ai-pdf2docx.py`: Main wrapper script and command-line interface.
* `phase1_pdf_to_json.py`: Contains the logic for PDF analysis and communication with Vertex AI.
* `phase2_json_to_docx.py`: Contains the logic for converting structured JSON to DOCX files.
* `.vscode/launch.json`: VS Code debugger configurations.
* `requirements.txt`: Python dependencies.
* `README.md`: This file.
